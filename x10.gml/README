#
#  This file is part of the X10 project (http://x10-lang.org).
#
#  This file is licensed to You under the Eclipse Public License (EPL);
#  You may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#      http://www.opensource.org/licenses/eclipse-1.0.php
#
#  (C) Copyright IBM Corporation 2006-2014.
#

============== X10 Global Matrix Library (GML) =================

The X10 Global Matrix Library implements double-precision dense and
sparse matrices, partitioned into blocks and distributed across
multiple places. Operations provided include cell-wise addition and
multiplication, matrix multiply (using the SUMMA algorithm) and 
others including various norm and distance metrics, sum and trace. 
High-level operations operate on entire matrices and are intended to 
support a programmer to write in a sequential style while fully 
exploiting available parallelism.

The library runs on both Native and Managed backends using the X10RT
transport; on Native X10 it can also run using a combination of X10RT
and Native MPI calls. It has been tested on clusters of multi-core x86
nodes connected with Infiniband and Ethernet and on the IBM Power775
system.

Some example application programs are provided, including Gaussian
non-negative matrix factorization, PageRank, linear regression and
logistic regression.


=============================================================
Instructions

1) System dependencies and settings
   * BLAS (basic linear algebra subprograms). 
     An optimized BLAS implementation is usually available on high-performance
     computing systems.
     If GML is built without BLAS enabled, an exception will be thrown if the
     corresponding linear algebra method is called.
     The environment variable DISABLE_BLAS controls this build option.

   * LAPACK - Linear Algebra PACKage (optional)
     LAPACK is used to find eigenvalues/vectors and solve linear equations.
     It is not required to build GML.  If GML is built without LAPACK enabled,
     an exception will be thrown if the corresponding solver method is called.
     The environment variable DISABLE_LAPACK controls this build option.
    
     In system_setting.mk, uncomment "add_lapack=yes" line to build GML with 
     LAPACK library.  
     Also make sure "lapack_path" and "lapack_name" are defined correctly.

     The file scripts/system_setting.mk defines build variables POST_CXXFLAGS
     and POST_LDFLAGS to compile against several commonly used BLAS and LAPACK 
     implementations, including:
      - NetLib reference implementation - http://netlib.org/blas
      - GotoBLAS2 - http://www.tacc.utexas.edu/tacc-projects/gotoblas2
      - IBM ESSL
      - Intel MKL
    
     If GML fails to compile against the BLAS implementation on your system,
     you can modify the values of POST_CXXFLAGS and POST_LDFLAGS accordingly.

   * Native-MPI backend:
     Uses x10rt=mpi with the corresponding MPI post-compiler.

   * Managed backend: Java SDK include path
     In system_setting.mk, check "JNI_LIBS" is defined correctly.


2) Build libraries
make help       -- Print help and make instruction for different transport and backend
make native     -- Make native backend library
make managed    -- Make managed backend library
make native_mpi -- Make native backend for MPI transport

Output
Include dir:  include/. 
Library dir:  lib/.
  Native backend         : native_gml.jar,     native_gml.so 
  Native MPI transport   : native_mpi_gml.jar, native_mpi_gml.so
  Managed backend        : managed_gml.jar

property files
  Native backend         : native_gml.properties
  Native MPI transport   : native_mpi_gml.properties
  Managed backend        : managed_gml.properties

3) Building applications
- Add [native|native_mpi|managed]_gml.jar in "-classpath" 
- Add [native|native_mpi|managed].properties in "-x10lib"
NOTE: Problem with specifying gml path name before jar and properties file names.
If using "~" in the path name, and compiler complains about not 
finding the library, replacing "~" with either absolute path or relative 
path of the gml library.

4) Running applications
- Native MPI transport
  No different from running MPI applications
  mpirun -np [number of processes] [app name] [args]
- Native backend on socket transport
  [absolute path]/X10Launcher -np [number of places] [app name] names] [args]
- Managed backend
  Add managed_gml.jar to "-classpath"
  Add gml library path to "-libpath"
  [absolute path]/X10Launcher -np [number of places] x10 -classpath [managed_gml.jar] -libpath [gml lib] [app name] [args] 
   

==============================================================
1) Matrix types:

The current implementation of X10 parallel matrix library supports
two matrix structures: dense and sparse, which are compatible with
column-major and CSC-LT format.
 
1.1) Basic types
  - Dense matrix: matrix elements are stored in column-major order in 
    a continuous memory space. 
  - Symmetric and triangular matrix. Current implementation supports
    the lower part of matrix data stored in column-major.
  - Sparse matrix: compatible to CSC-LT format, which stores non-zero
    element values and corresponding indexes in the column-major order.


Matrix data can be partitioned into blocks in a grid. Each 
block stores data in dense or sparse structure.
1.2) Block-partitioned matrix
  - Dense block matrix: matrix is partitioned in (m x n) blocks, where m 
    is number of row blocks and n is number of column blocks. Each block 
    uses dense matrix format to store matrix data.
  - Sparse block matrix: similar to dense block matrix, but uses sparse 
    matrix format to store data of all matrix blocks.


Partitioned matrix can distribute blocks to all places.  The current 
implementation is restricted to 1-to-1 mapping between blocks and places. 
Future versions may remove this restriction.

1.3) Distributed matrix
  - Distributed dense matrix: matrix is partitioned and each place is 
    assigned with one dense block only.
  - Distributed sparse matrix: similar to distributed dense block.

2) Matrix operations

Some commonly used operations are implemented, including cell-wise add, 
subtract, multiply, division, cell-wise sum, Euclidean distance, max norm,
trace and matrix multiply. 

Common rules:
 - Result of a matrix operation is stored in the invocation object, which 
   is also the returned value.  This allows chain operations.
 - Most of matrix operations have the results stored in dense matrix 
   structures, which includes dense matrix, dense block matrix and
   distributed dense matrix.  The sparse matrix is not used to store 
   results. The exception is scaling operations for sparse matrix. 
 - Operands of must have the same distribution if they are distributed matrix
 

1.1) Cell-wise operations

Cell-wise operations are defined as add-on operations, which means the 
output also serves as input operand in the operation.  

  - scaling
  - add
  - subtract
  - multiply
  - divide
  - sum

1.2) Matrix properties
  - Euclidean distance
  - max norm
  - trace

1.3) Matrix multiplication
    - multiply
    - multiply with first operand transposed
    - multiply with second operand transposed



